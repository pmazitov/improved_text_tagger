{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improved_tagger.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "qzI43gds8iKy"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WA_ON4HFV4Na"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/igorbrigadir/stopwords/master/en/alir3z4.txt -O 'stopwords.txt'\n",
        "!wget https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt -L -O 'popular-words.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYAhrO9BBFUJ",
        "outputId": "5002f54d-5a92-4a12-a17c-019e3379494e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-15 14:27:25--  https://raw.githubusercontent.com/igorbrigadir/stopwords/master/en/alir3z4.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7678 (7.5K) [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "\rstopwords.txt         0%[                    ]       0  --.-KB/s               \rstopwords.txt       100%[===================>]   7.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-15 14:27:25 (60.5 MB/s) - ‘stopwords.txt’ saved [7678/7678]\n",
            "\n",
            "--2022-05-15 14:27:25--  https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english-no-swears.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75153 (73K) [text/plain]\n",
            "Saving to: ‘popular-words.txt’\n",
            "\n",
            "popular-words.txt   100%[===================>]  73.39K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-05-15 14:27:25 (10.7 MB/s) - ‘popular-words.txt’ saved [75153/75153]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmdalt1PBW4G",
        "outputId": "f30e9ce7-355d-46c1-9a71-777d8d20e670"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "popular-words.txt  sample_data\tstopwords.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "\n",
        "# https://github.com/igorbrigadir/stopwords/blob/master/en/alir3z4.txt\n",
        "with open('stopwords.txt') as stop_words_file:\n",
        "    STOP_WORDS_ALIR3Z4 = stop_words_file.read().split('\\n')\n",
        "\n",
        "# https://github.com/first20hours/google-10000-english/blob/master/google-10000-english-no-swears.txt\n",
        "with open('popular-words.txt') as popular_words_file:\n",
        "    POPULAR_WORDS = popular_words_file.read().split('\\n')\n",
        "\n",
        "POPULAR_TAGS = list(set(POPULAR_WORDS) - set(STOP_WORDS_ALIR3Z4))\n",
        "\n",
        "\n",
        "def extract_words(text: str, alphabet: str, min_length: int = 3, stop_words: List[str] = None):\n",
        "    \"\"\"Split text into word.\"\"\"\n",
        "    stop_words = stop_words or []\n",
        "\n",
        "    # filter symbols\n",
        "    text = ''.join(\n",
        "        (c if c in alphabet else ' ')\n",
        "        for c in text.lower()\n",
        "    )\n",
        "\n",
        "    # split to words\n",
        "    words = text.split()\n",
        "\n",
        "    # filter words\n",
        "    return [\n",
        "        word\n",
        "        for word in words\n",
        "        if word not in stop_words and len(word) >= min_length\n",
        "    ]\n",
        "\n",
        "\n",
        "class BaseTagger(ABC):\n",
        "    @abstractmethod\n",
        "    def get_tags(self, texts: List[str]) -> List[List[str]]:\n",
        "        \"\"\"['Text1', 'Text2', ...] -> [['text1_tag1', 'text1_tag2', ...], ...]\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "class BaseSeparateTagger(BaseTagger, ABC):\n",
        "    @abstractmethod\n",
        "    def get_tags_from_text(self, text: str) -> List[str]:\n",
        "        \"\"\"'Text' -> ['text_tag1', 'text_tag2', ...]\"\"\"\n",
        "        ...\n",
        "\n",
        "    def get_tags(self, texts: List[str]) -> List[List[str]]:\n",
        "        result = []\n",
        "        for text in texts:\n",
        "            tags = self.get_tags_from_text(text)\n",
        "            result.append(tags)\n",
        "        return result\n",
        "\n",
        "\n",
        "class MostFrequentWordsTagger(BaseSeparateTagger):\n",
        "    default_stop_words = STOP_WORDS_ALIR3Z4\n",
        "    words_alphabet = 'abcdefghijklmnopqrstuvwxyz-\\''\n",
        "\n",
        "    def __init__(self, stop_words: list = None, max_tags_per_text: int = 5):\n",
        "        self.stop_words = stop_words or self.default_stop_words\n",
        "        self.max_tags_per_text = max_tags_per_text\n",
        "\n",
        "    def get_tags_from_text(self, text: str) -> List[str]:\n",
        "        words = extract_words(text, alphabet=self.words_alphabet, min_length=3, stop_words=self.stop_words)\n",
        "        words_counter = Counter(words)\n",
        "\n",
        "        # TODO improve heuristics\n",
        "        tags = []\n",
        "        result = words_counter.most_common()\n",
        "        if len(result) == 0:\n",
        "            return []\n",
        "\n",
        "        word, max_count = result[0]\n",
        "        i = 0\n",
        "        while result[i][1] == max_count:\n",
        "            tags.append(result[i][0])\n",
        "            i += 1\n",
        "\n",
        "        return tags[:self.max_tags_per_text]"
      ],
      "metadata": {
        "id": "wB-8-RlRZGF2"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "\"We are arriving in Rome via cruise ship on June 3, 2023. We are taking the transfer from the cruise ship that leaves at 7am. Our biggest priorities are the Vatican Museum/St Peter's Basilica and the Colloseum. My plan was to book a tour(or skip the line) at the Vatican starting at 9, thinking to stay 3 hours. Then taxi to the Colloseum, again tour or skip the line, tour at 1pm, staying 1.5 hours, possibly visiting the Forum (maybe 30 minutes). Is there any hope of seeing Trevi Fountain and the Pantheon as well? My plan would be to take a taxi from Colloseum to Trevi fountain, and then taxi to our transfer point near the Vatican where we have to be at 5pm, so that we maximize the time we have. Worst case scenario, we can see our 2 priority stops and nix the stop at Trevi fountain if we are running late. But wondering if this plan sounds feasible? And is there any small hope that we might be able to squish in a quick visit to Trevi fountain? Thanks for all and any advice! Jacqueline\",\n",
        "\"Crocodile, (order Crocodylia, or Crocodilia), any of 23 species of generally large, ponderous, amphibious animals of lizard-like appearance and carnivorous habit belonging to the reptile order Crocodylia. Crocodiles have powerful jaws with many conical teeth and short legs with clawed webbed toes. They share a unique body form that allows the eyes, ears, and nostrils to be above the water surface while most of the animal is hidden below. The tail is long and massive, and the skin is thick and plated. Crocodiles are a living link with the dinosaur-like reptiles of prehistoric times and are the nearest living relatives of the birds. A large variety of crocodilian fossils have been discovered that date back 200 million years to the Late Triassic Epoch. Fossil evidence also suggests that three major radiations occurred. Only one of the four suborders of crocodiles has survived to modern times. The order Crocodylia includes the “true crocodiles,” alligators, caimans, and gavials.\",\n",
        "\"When Netflix last month revealed that it had lost customers for the first time in a decade, you might have expected competing streaming services to be jubilant. In the two weeks after cancelling their subscription to Netflix, 87 per cent of subscribers had not signed up to a rival service, according to the analysis, which is based on around 3mn US internet users. Richard Broughton, director of research at Ampere, said that although there was an increase in churn rates at the start of the year, “there is no strong evidence to suggest that customers are being pulled away due to interest in other [streaming video] services”. The data suggest that a combination of higher inflation and a weakening stock market prompted consumers to tighten their budgets. Many of those that left the streaming service were aged between 18 and 24, or in households with an annual income of less than $15,000, according to the analysis. About 49 per cent of the poorest households surveyed said they had a Netflix subscription in the first quarter, down from about 56.2 per cent in the previous year.\",\n",
        "\"Python language is incredibly easy to use and learn for new beginners and newcomers. The python language is one of the most accessible programming languages available because it has simplified syntax and not complicated, which gives more emphasis on natural language. Due to its ease of learning and usage, python codes can be easily written and executed much faster than other programming languages. When Guido van Rossum was creating python in the 1980s, he made sure to design it to be a general-purpose language. One of the main reasons for the popularity of python would be its simplicity in syntax so that it could be easily read and understood even by amateur developers also. One can also quickly experiment by changing the code base of python because it is an interpreted language which makes it even more popular among all kinds of developers.\",\n",
        "]"
      ],
      "metadata": {
        "id": "3RS5jvPmDSN4"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "class MostFrequentWordsTagger_improved:\n",
        "    default_stop_words = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))\n",
        "\n",
        "    def __init__(self, stop_words: list = None, max_tags_per_text: int = 5):\n",
        "        self.stop_words = stop_words or self.default_stop_words\n",
        "        self.max_tags_per_text = max_tags_per_text\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.corpus_vectorized = None\n",
        "\n",
        "    def get_tags(self, texts: List[str], k: int = 5,\n",
        "                 thr: float = 0.8, min_tag_len = 4) -> List[List[str]]:\n",
        "        new_corpus = [' '.join([w for w in nltk.tokenize.word_tokenize(text.lower())\n",
        "                                if w not in self.stop_words]) for text in texts]\n",
        "        self.corpus_vectorized = self.vectorizer.fit_transform(new_corpus)\n",
        "        all_words = self.vectorizer.get_feature_names_out()\n",
        "        result = []\n",
        "        for i in range(len(texts)):\n",
        "            tfidf_vector = self.corpus_vectorized[i].toarray()[0]\n",
        "            best_k_ixes = np.argpartition(tfidf_vector, -k)[-k:]\n",
        "            scores = tfidf_vector[best_k_ixes]\n",
        "            best_score = max(scores)\n",
        "            mask = scores / best_score > thr\n",
        "            best_ixes = best_k_ixes[mask]\n",
        "\n",
        "            tags = [w for w in all_words[best_ixes] if len(w) >= min_tag_len]\n",
        "            result.append(tags)\n",
        "        return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J1rLyiZCB8L",
        "outputId": "bdeba27c-fca8-496f-ce03-4784eb9fbe69"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MostFrequentWordsTagger().get_tags(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhDeb_zBCCBC",
        "outputId": "402b8fa1-205d-4dbc-cee6-f640d7e05ca1"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['trevi', 'fountain'], ['crocodiles'], ['netflix', 'streaming', 'cent'], ['python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MostFrequentWordsTagger_improved().get_tags(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tWPU7SFCCDQ",
        "outputId": "1247e92e-5c2d-46be-b829-20477383f483"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['fountain', 'trevi'], ['crocodiles'], ['streaming', 'cent', 'netflix'], ['language', 'python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that with improved tagger we get better results for 4-d document. Also we now don't use stopwords from stopwords.txt but import them from nltk. Also our tagger is more flexible and controllable."
      ],
      "metadata": {
        "id": "fo257JEbaDjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ry_Bq4NNCCFg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}